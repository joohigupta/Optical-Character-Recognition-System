{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!cp -r ../input/efficientnetpytorch/ ./efficientnetpytorch\n!pip install ./efficientnetpytorch/\n!rm -r ./efficientnetpytorch/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%config Completer.use_jedi = False\n\nimport os\nimport json\nimport functools\n\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torchvision import transforms\nfrom efficientnet_pytorch import EfficientNet\nimport numpy as np\nfrom tqdm import tqdm\nimport pandas as pd\nfrom PIL import Image\nimport albumentations as A\nimport sklearn.metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mode = 'test'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_GRAPHEME_ROOT = 168\nNUM_VOWEL_DIACRITIC = 11\nNUM_CONSONANT_DIACRITIC = 8\nclass_map = pd.read_csv('../input/bengaliai-cv19/class_map.csv')\ngrapheme_root = class_map[class_map['component_type'] == 'grapheme_root']\nvowel_diacritic = class_map[class_map['component_type'] == 'vowel_diacritic']\nconsonant_diacritic = class_map[class_map['component_type'] == 'consonant_diacritic']\ngrapheme_root_list = grapheme_root['component'].tolist()\nvowel_diacritic_list = vowel_diacritic['component'].tolist()\nconsonant_diacritic_list = consonant_diacritic['component'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BengalModel(nn.Module):\n    def __init__(self, backbone, hidden_size=2560, class_num=168*11*7):\n        super(BengalModel, self).__init__()\n        self.backbone = backbone\n        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(hidden_size, class_num)\n        self.ln = nn.LayerNorm(hidden_size)\n\n        \n    def forward(self, inputs):\n        bs = inputs.shape[0]\n        feature = self.backbone.extract_features(inputs)\n        feature_vector = self._avg_pooling(feature)\n        feature_vector = feature_vector.view(bs, -1)\n        feature_vector = self.ln(feature_vector)\n\n        out = self.fc(feature_vector)\n        return out   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Albumentations:\n    def __init__(self, augmentations):\n        self.augmentations = A.Compose(augmentations)\n    \n    def __call__(self, image):\n        image = self.augmentations(image=image)['image']\n        return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestGraphemeDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, images, transform=None):\n        self.images = images\n        self.transform = transform\n            \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        np_image = self.images[idx].copy()\n        out_image = self.transform(np_image)\n        return out_image, idx\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_to_grapheme(grapheme_root, vowel_diacritic, consonant_diacritic):\n    if consonant_diacritic == 0:\n        if vowel_diacritic == 0:\n            return grapheme_root_list[grapheme_root]\n        else:\n            return grapheme_root_list[grapheme_root] + vowel_diacritic_list[vowel_diacritic]\n    elif consonant_diacritic == 1:\n        if vowel_diacritic == 0:\n            return grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic]\n        else:\n            return grapheme_root_list[grapheme_root] + vowel_diacritic_list[vowel_diacritic] + consonant_diacritic_list[consonant_diacritic]\n    elif consonant_diacritic == 2:\n        if vowel_diacritic == 0:\n            return consonant_diacritic_list[consonant_diacritic] + grapheme_root_list[grapheme_root]\n        else:\n            return consonant_diacritic_list[consonant_diacritic] + grapheme_root_list[grapheme_root] + vowel_diacritic_list[vowel_diacritic]\n    elif consonant_diacritic == 3:\n        if vowel_diacritic == 0:\n            return consonant_diacritic_list[consonant_diacritic][:2] + grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic][1:]\n        else:\n            return consonant_diacritic_list[consonant_diacritic][:2] + grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic][1:] + vowel_diacritic_list[vowel_diacritic]\n    elif consonant_diacritic == 4:\n        if vowel_diacritic == 0:\n            return grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic]\n        else:\n            if grapheme_root == 123 and vowel_diacritic == 1:\n                return grapheme_root_list[grapheme_root] + '\\u200d' + consonant_diacritic_list[consonant_diacritic] + vowel_diacritic_list[vowel_diacritic]\n            return grapheme_root_list[grapheme_root]  + consonant_diacritic_list[consonant_diacritic] + vowel_diacritic_list[vowel_diacritic]\n    elif consonant_diacritic == 5:\n        if vowel_diacritic == 0:\n            return grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic]\n        else:\n            return grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic] + vowel_diacritic_list[vowel_diacritic]\n    elif consonant_diacritic == 6:\n        if vowel_diacritic == 0:\n            return grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic]\n        else:\n            return grapheme_root_list[grapheme_root] + consonant_diacritic_list[consonant_diacritic] + vowel_diacritic_list[vowel_diacritic]\n    elif consonant_diacritic == 7:\n        if vowel_diacritic == 0:\n            return consonant_diacritic_list[2] + grapheme_root_list[grapheme_root] + consonant_diacritic_list[2][::-1]\n        else:\n            return consonant_diacritic_list[2] + grapheme_root_list[grapheme_root] + consonant_diacritic_list[2][::-1] + vowel_diacritic_list[vowel_diacritic]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResnetGenerator(nn.Module):\n    \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n    We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n    \"\"\"\n\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n        \"\"\"Construct a Resnet-based generator\n        Parameters:\n            input_nc (int)      -- the number of channels in input images\n            output_nc (int)     -- the number of channels in output images\n            ngf (int)           -- the number of filters in the last conv layer\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers\n            n_blocks (int)      -- the number of ResNet blocks\n            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n        \"\"\"\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):  # add downsampling layers\n            mult = 2 ** i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2 ** n_downsampling\n        for i in range(n_blocks):       # add ResNet blocks\n\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        for i in range(n_downsampling):  # add upsampling layers\n            mult = 2 ** (n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        \"\"\"Standard forward\"\"\"\n        return self.model(input)\n\n\nclass ResnetBlock(nn.Module):\n    \"\"\"Define a Resnet block\"\"\"\n\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        \"\"\"Initialize the Resnet block\n        A resnet block is a conv block with skip connections\n        We construct a conv block with build_conv_block function,\n        and implement skip connections in <forward> function.\n        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n        \"\"\"\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        \"\"\"Construct a convolutional block.\n        Parameters:\n            dim (int)           -- the number of channels in the conv layer.\n            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers.\n            use_bias (bool)     -- if the conv layer uses bias or not\n        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n        \"\"\"\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        \"\"\"Forward function (with skip connections)\"\"\"\n        out = x + self.conv_block(x)  # add skip connections\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super(EnsembleModel, self).__init__()\n        self.model1 = models[0]\n        self.model2 = models[1]\n        \n    def forward(self, images):\n\n        return self.model1(images), self.model2(images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_merc(classifier_load_path1, generator_load_path1, classifier_load_path2, generator_load_path2, images):\n    MEAN = [0.5, 0.5, 0.5]\n    STD = [0.5, 0.5, 0.5]\n    IMG_HEIGHT = 224\n    IMG_WIDTH = 224\n    BATCH_SIZE = 32\n    norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n    generator_b1 = ResnetGenerator(input_nc=3, output_nc=3, ngf=64, norm_layer=norm_layer, use_dropout=False, n_blocks=9)\n    backbone1 = EfficientNet.from_name('efficientnet-b0')\n    classifier1 = BengalModel(backbone1, hidden_size=1280, class_num=NUM_GRAPHEME_ROOT*NUM_VOWEL_DIACRITIC*NUM_CONSONANT_DIACRITIC)\n\n    classifier1.load_state_dict(torch.load(classifier_load_path1))\n    generator_b1.load_state_dict(torch.load(generator_load_path1))\n    model1 = nn.Sequential(generator_b1, classifier1)\n    \n    generator_b2 = ResnetGenerator(input_nc=3, output_nc=3, ngf=64, norm_layer=norm_layer, use_dropout=False, n_blocks=9)\n    backbone2 = EfficientNet.from_name('efficientnet-b0')\n    classifier2 = BengalModel(backbone2, hidden_size=1280, class_num=NUM_GRAPHEME_ROOT*NUM_VOWEL_DIACRITIC*NUM_CONSONANT_DIACRITIC)\n\n    classifier2.load_state_dict(torch.load(classifier_load_path2))\n    generator_b2.load_state_dict(torch.load(generator_load_path2))\n    model2 = nn.Sequential(generator_b2, classifier2)\n    model = EnsembleModel([model1, model2])\n    model.to(device)\n    model.eval()\n    \n    grapheme_root_map = np.zeros((NUM_GRAPHEME_ROOT*NUM_VOWEL_DIACRITIC*NUM_CONSONANT_DIACRITIC, ), dtype=np.int64)\n    vowel_diacritic_map = np.zeros((NUM_GRAPHEME_ROOT*NUM_VOWEL_DIACRITIC*NUM_CONSONANT_DIACRITIC, ), dtype=np.int64)\n    consonant_diacritic_map = np.zeros((NUM_GRAPHEME_ROOT*NUM_VOWEL_DIACRITIC*NUM_CONSONANT_DIACRITIC, ), dtype=np.int64)\n    for grapheme_root in range(168):\n        for vowel_diacritic in range(11):\n            for consonant_diacritic in range(8):\n                i = (grapheme_root*NUM_VOWEL_DIACRITIC + vowel_diacritic)*NUM_CONSONANT_DIACRITIC + consonant_diacritic\n                grapheme_root_map[i] = grapheme_root\n                vowel_diacritic_map[i] = vowel_diacritic\n                consonant_diacritic_map[i] = consonant_diacritic\n    \n    preprocess = [\n        A.CenterCrop(height=137, width=IMG_WIDTH),\n        A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, always_apply=True),\n    ]\n\n    transform = transforms.Compose([\n        np.uint8,\n        transforms.Lambda(lambda x: np.array([x, x, x]).transpose((1, 2, 0)) ),\n        np.uint8,\n        Albumentations(preprocess),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=MEAN, std=STD),\n    #     transforms.ToPILImage(),\n    ])\n    \n    dataset = TestGraphemeDataset(images, transform)\n    loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n    def out2pred(out):\n        out1, out2 = out\n        softmax = nn.Softmax(dim=1)\n        out1 = softmax(out1)\n        out2 = softmax(out2)\n        out = out1+out2\n        \n        box_out1 = out1.reshape((-1, NUM_GRAPHEME_ROOT, NUM_VOWEL_DIACRITIC, NUM_CONSONANT_DIACRITIC))\n        grapheme_root_out = box_out1.sum(dim=(2, 3))\n        vowel_diacritic_out = box_out1.sum(dim=(1, 3))\n        consonant_diacritic_out = box_out1.sum(dim=(1, 2))\n        box_out2 = out2.reshape((-1, NUM_GRAPHEME_ROOT, NUM_VOWEL_DIACRITIC, NUM_CONSONANT_DIACRITIC))\n        grapheme_root_out += box_out2.sum(dim=(2, 3))\n        vowel_diacritic_out += box_out2.sum(dim=(1, 3))\n        consonant_diacritic_out += box_out2.sum(dim=(1, 2))\n        grapheme_root_preds = grapheme_root_out.argmax(dim=1).cpu().numpy()\n        vowel_diacritic_preds = vowel_diacritic_out.argmax(dim=1).cpu().numpy()\n        consonant_diacritic_preds = consonant_diacritic_out.argmax(dim=1).cpu().numpy()\n        preds = (grapheme_root_preds*NUM_VOWEL_DIACRITIC+vowel_diacritic_preds)*NUM_CONSONANT_DIACRITIC+consonant_diacritic_preds\n#         confidences, preds = out.max(dim=1)\n\n#         confidences = confidences.cpu().numpy()\n#         preds = preds.cpu().numpy()\n#         grapheme_root_preds = grapheme_root_map[preds]\n#         vowel_diacritic_preds = vowel_diacritic_map[preds]\n#         consonant_diacritic_preds = consonant_diacritic_map[preds]\n        ret = []\n        for p, g, v, co in zip(preds, grapheme_root_preds, vowel_diacritic_preds, consonant_diacritic_preds):\n            x = {\n                'pred': p,\n#                 'confidence': c,\n                'grapheme_root': g,\n                'vowel_diacritic': v,\n                'consonant_diacritic': co,\n                'grapheme': label_to_grapheme(g, v, co),\n            }\n            ret.append(x)\n        return ret\n        \n    \n    return model, loader, out2pred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluator(model, loader, out2pred):\n    ret = []\n    model.eval()\n    softmax = nn.Softmax(dim=1)\n    for (images, idx) in tqdm(loader):\n        images = images.to(device)\n        with torch.no_grad():\n            out = model(images)\n            ret += out2pred(out)\n    return ret","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_images(path):\n    image_df = pd.read_parquet(path)\n    images = image_df.iloc[:, 1:].values.reshape(-1, 137, 236)\n    return images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merc_result = []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = load_images('../input/bengaliai-cv19/{}_image_data_0.parquet'.format(mode))\nmodel, loader, out2pred = create_merc('../input/cyclegan-classifier-results/best.pth', '../input/cyclegan-training-results/generator.pth', '../input/cyclegan-classifier-results/best.pth', '../input/cyclegan-training-results/generator.pth', images)\nmerc_result += evaluator(model, loader, out2pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = load_images('../input/bengaliai-cv19/{}_image_data_1.parquet'.format(mode))\nmodel, loader, out2pred = create_merc('../input/cyclegan-classifier-results/best.pth', '../input/cyclegan-training-results/generator.pth', '../input/cyclegan-classifier-results/best.pth', '../input/cyclegan-training-results/generator.pth', images)\nmerc_result += evaluator(model, loader, out2pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = load_images('../input/bengaliai-cv19/{}_image_data_2.parquet'.format(mode))\nmodel, loader, out2pred = create_merc('../input/cyclegan-classifier-results/best.pth', '../input/cyclegan-training-results/generator.pth', '../input/cyclegan-classifier-results/best.pth', '../input/cyclegan-training-results/generator.pth', images)\nmerc_result += evaluator(model, loader, out2pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = load_images('../input/bengaliai-cv19/{}_image_data_3.parquet'.format(mode))\nmodel, loader, out2pred = create_merc('../input/cyclegan-classifier-results/best.pth', '../input/cyclegan-training-results/generator.pth', '../input/cyclegan-classifier-results/best.pth', '../input/cyclegan-training-results/generator.pth', images)\nmerc_result += evaluator(model, loader, out2pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merc_result_df = pd.DataFrame(merc_result)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_inference(merc_result_df):\n    ret = []\n    for merc_row in merc_result_df.itertuples():\n        inference = {}\n        inference['grapheme_root'] = merc_row.grapheme_root\n        inference['vowel_diacritic'] = merc_row.vowel_diacritic\n        inference['consonant_diacritic'] = merc_row.consonant_diacritic\n        inference['grapheme'] = merc_row.grapheme\n        ret.append(inference)\n        \n    return pd.DataFrame(ret)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inference = create_inference(merc_result_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit(inference):\n    row_id_list = []\n    target_list = []\n    for i, row in inference.iterrows():\n        row_id_list.append('Test_{}_grapheme_root'.format(i))\n        target_list.append(row.grapheme_root)\n        row_id_list.append('Test_{}_vowel_diacritic'.format(i))\n        target_list.append(row.vowel_diacritic)\n        row_id_list.append('Test_{}_consonant_diacritic'.format(i))\n        if row.consonant_diacritic == 7:\n            target_list.append(2)\n        else:\n            target_list.append(row.consonant_diacritic)\n    raw_submission = {\n        'row_id': row_id_list,\n        'target': target_list\n    }\n    submission = pd.DataFrame(raw_submission)\n    display(submission)\n    submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit(inference)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}